{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1Vt7RZOQva9x8jZF0VXoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeandroCoelhos/estudos_datascience/blob/main/LGBM/LGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MUkutTAtcs3n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vantagens do LGBM\n",
        "\n",
        "Otimiza√ß√£o em velocidade e uso de mem√≥ria\n",
        "\n",
        "Muitas ferramentas de boosting usam algoritmos baseados em pr√©-ordena√ß√£o [2, 3] (por exemplo, algoritmo padr√£o em xgboost) para aprendizado de √°rvores de decis√£o. √â uma solu√ß√£o simples, mas n√£o f√°cil de otimizar.\n",
        "\n",
        "O LightGBM usa algoritmos baseados em histograma [4, 5, 6] , que agrupam valores de recursos cont√≠nuos (atributos) em compartimentos discretos. Isso acelera o treinamento e reduz o uso de mem√≥ria. As vantagens dos algoritmos baseados em histogramas incluem o seguinte:\n",
        "\n",
        "    Custo reduzido de c√°lculo do ganho para cada divis√£o\n",
        "\n",
        "        Algoritmos baseados em pr√©-ordena√ß√£o t√™m complexidade de tempoO(#data)\n",
        "\n",
        "        Calcular o histograma tem complexidade de tempo O(#data), mas isso envolve apenas uma opera√ß√£o de soma r√°pida. Uma vez que o histograma √© constru√≠do, um algoritmo baseado em histograma tem complexidade O(#bins)de tempo e #bins√© muito menor que #data.\n",
        "\n",
        "    Use a subtra√ß√£o do histograma para acelerar ainda mais\n",
        "\n",
        "        Para obter os histogramas de uma folha em uma √°rvore bin√°ria, use a subtra√ß√£o do histograma de seu pai e seu vizinho\n",
        "\n",
        "        Portanto, ele precisa construir histogramas para apenas uma folha (com menor #dataque sua vizinha). Ele ent√£o pode obter histogramas de seu vizinho por subtra√ß√£o de histograma com pequeno custo ( O(#bins))\n",
        "\n",
        "    Reduza o uso de mem√≥ria\n",
        "\n",
        "        Substitui valores cont√≠nuos por compartimentos discretos. Se #binsfor pequeno, pode usar o tipo de dados pequeno, por exemplo, uint8_t, para armazenar dados de treinamento\n",
        "\n",
        "        N√£o h√° necessidade de armazenar informa√ß√µes adicionais para valores de recursos de pr√©-classifica√ß√£o\n",
        "\n",
        "    Reduza o custo de comunica√ß√£o para aprendizado distribu√≠do\n",
        "\n"
      ],
      "metadata": {
        "id": "Spqph2LUhi7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suporte a variaveis categ√≥ricas sem fazer one hot\n",
        "* categorical_feature"
      ],
      "metadata": {
        "id": "6_-WjnnrgBlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Par√¢metros\n",
        "num_leaves, default =31, type = int, aliases:num_leaf,max_leaves,max_leaf,max_leaf_nodes, restri√ß√µes:1 < num_leaves <= 131072\n",
        "\n",
        "    n√∫mero m√°ximo de folhas em uma √°rvore\n",
        "<hr>\n",
        "\n",
        "tree_learner , default =serial, type = enum, op√ß√µes:serial,feature,data,voting, aliases:tree,tree_type,tree_learner_type\n",
        "\n",
        "    serial, aprendizado de √°rvore de m√°quina √∫nica\n",
        "\n",
        "    feature, recurso de aprendizado de √°rvore paralela, aliases:feature_parallel\n",
        "\n",
        "    data, aprendiz de √°rvore paralela de dados, aliases:data_parallel\n",
        "\n",
        "    voting, aluno de √°rvore paralela de vota√ß√£o, aliases:voting_parallel\n",
        "\n",
        "    consulte o Guia de Aprendizagem Distribu√≠da para obter mais detalhes - https://lightgbm-readthedocs-io.translate.goog/en/v3.3.2/Parallel-Learning-Guide.html?_x_tr_sl=en&_x_tr_tl=pt&_x_tr_hl=pt-BR&_x_tr_pto=sc\n",
        "<hr>\n",
        "\n",
        "num_threads üîóÔ∏é , default =0, type = int, aliases:num_thread,nthread,nthreads,n_jobs\n",
        "\n",
        "    n√∫mero de threads para LightGBM\n",
        "\n",
        "    0significa o n√∫mero padr√£o de threads no OpenMP\n",
        "\n",
        "    para a melhor velocidade, defina isso para o n√∫mero de n√∫cleos de CPU reais , n√£o o n√∫mero de threads (a maioria das CPUs usa hyper-threading para gerar 2 threads por n√∫cleo de CPU)\n",
        "\n",
        "    n√£o defina muito grande se seu conjunto de dados for pequeno (por exemplo, n√£o use 64 threads para um conjunto de dados com 10.000 linhas)\n",
        "\n",
        "    esteja ciente de que um gerenciador de tarefas ou qualquer ferramenta de monitoramento de CPU semelhante pode relatar que os n√∫cleos n√£o est√£o sendo totalmente utilizados. Isto √© normal\n",
        "\n",
        "    para aprendizado distribu√≠do, n√£o use todos os n√∫cleos da CPU, pois isso causar√° um desempenho ruim para a comunica√ß√£o da rede\n",
        "\n",
        "    Nota : por favor , n√£o altere isso durante o treinamento, especialmente ao executar v√°rios trabalhos simultaneamente por pacotes externos, caso contr√°rio, poder√° causar erros indesej√°veis\n",
        "\n",
        "\n",
        "\n",
        "device_type üîóÔ∏é , default =cpu, type = enum, op√ß√µes:cpu,gpu,cuda, aliases:device\n",
        "\n",
        "    dispositivo para o aprendizado em √°rvore, voc√™ pode usar a GPU para obter o aprendizado mais r√°pido\n",
        "\n",
        "    Nota : recomenda-se usar o menor max_bin(por exemplo, 63) para obter a melhor velocidade\n",
        "\n",
        "    Nota : para a velocidade mais r√°pida, a GPU usa o ponto flutuante de 32 bits para resumir por padr√£o, portanto, isso pode afetar a precis√£o de algumas tarefas. Voc√™ pode configurar gpu_use_dp=truepara habilitar o ponto flutuante de 64 bits, mas isso retardar√° o treinamento\n",
        "\n",
        "    Nota : consulte o Guia de instala√ß√£o para construir LightGBM com suporte a GPU\n",
        "\n",
        "<hr>\n",
        "seed üîóÔ∏é , default =None, type = int, aliases:random_seed,random_state\n",
        "\n",
        "    esta semente √© usada para gerar outras sementes, por exemplo data_random_seed, feature_fraction_seed, etc.\n",
        "\n",
        "    por padr√£o, esta semente n√£o √© usada em favor dos valores padr√£o de outras sementes\n",
        "\n",
        "    esta semente tem prioridade menor em compara√ß√£o com outras sementes, o que significa que ela ser√° substitu√≠da, se voc√™ definir outras sementes explicitamente\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZbHHxnRln5jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hiperpar√£metros de Controle de Aprendizagem\n",
        "\n",
        "<hr>\n",
        "max_depth üîóÔ∏é , padr√£o =-1, tipo = int\n",
        "\n",
        "    limitar a profundidade m√°xima para o modelo de √°rvore. Isso √© usado para lidar com o ajuste excessivo quando #data√© pequeno. A √°rvore ainda cresce em folha\n",
        "\n",
        "    <= 0significa sem limite\n",
        "\n",
        "\n",
        "<hr>\n",
        "min_data_in_leaf üîóÔ∏é , default =20, type = int, aliases:min_data_per_leaf,min_data,min_child_samples,min_samples_leaf, restri√ß√µes:min_data_in_leaf >= 0\n",
        "\n",
        "    n√∫mero m√≠nimo de dados em uma folha. Pode ser usado para lidar com sobre-ajuste\n",
        "\n",
        "    Nota : esta √© uma aproxima√ß√£o baseada no Hessian, ent√£o ocasionalmente voc√™ pode observar divis√µes que produzem n√≥s folha que t√™m menos do que este n√∫mero de observa√ß√µes\n",
        "\n",
        "\n",
        "<hr>\n",
        "<b>Desbalanceamento</b>\n",
        "\n",
        "pos_bagging_fraction üîóÔ∏é , default =1.0, type = double, aliases:pos_sub_row,pos_subsample,pos_bagging, restri√ß√µes:0.0 < pos_bagging_fraction <= 1.0\n",
        "\n",
        "    usado apenas em binaryaplica√ß√µes\n",
        "\n",
        "    usado para o problema de classifica√ß√£o bin√°ria desequilibrada, amostrar√° aleatoriamente amostras positivas em ensacamento#pos_samples * pos_bagging_fraction\n",
        "\n",
        "    deve ser usado em conjunto comneg_bagging_fraction\n",
        "\n",
        "    defina isso 1.0para desabilitar\n",
        "\n",
        "    Nota : para habilitar isso, voc√™ precisa definir bagging_freqe neg_bagging_fractiontamb√©m\n",
        "\n",
        "    Nota : se ambos pos_bagging_fractione neg_bagging_fractionestiverem definidos como 1.0, o ensacamento balanceado ser√° desabilitado\n",
        "\n",
        "    Nota : se o ensacamento balanceado estiver habilitado, bagging_fractionser√° ignorado\n",
        "\n",
        "<hr>\n",
        "neg_bagging_fraction üîóÔ∏é , default =1.0, type = double, aliases:neg_sub_row,neg_subsample,neg_bagging, restri√ß√µes:0.0 < neg_bagging_fraction <= 1.0\n",
        "\n",
        "    usado apenas em binaryaplica√ß√µes\n",
        "\n",
        "    usado para o problema de classifica√ß√£o bin√°ria desequilibrada, amostrar√° aleatoriamente amostras negativas no ensacamento#neg_samples * neg_bagging_fraction\n",
        "\n",
        "    deve ser usado em conjunto compos_bagging_fraction\n",
        "\n",
        "    defina isso 1.0para desabilitar\n",
        "\n",
        "    Nota : para habilitar isso, voc√™ precisa definir bagging_freqe pos_bagging_fractiontamb√©m\n",
        "\n",
        "    Nota : se ambos pos_bagging_fractione neg_bagging_fractionestiverem definidos como 1.0, o ensacamento balanceado ser√° desabilitado\n",
        "\n",
        "    Nota : se o ensacamento balanceado estiver habilitado, bagging_fractionser√° ignorado\n",
        "\n",
        "\n",
        "<hr>\n",
        "bagging_freq üîóÔ∏é , default =0, type = int, aliases:subsample_freq\n",
        "\n",
        "    frequ√™ncia de ensacamento\n",
        "\n",
        "    0significa desabilitar o ensacamento; ksignifica realizar o ensacamento a cada kitera√ß√£o. A cada k-th itera√ß√£o, o LightGBM selecionar√° aleatoriamente os dados a serem usados ‚Äã‚Äãnas pr√≥ximas itera√ß√µesbagging_fraction * 100 %k\n",
        "\n",
        "    Nota : para habilitar o ensacamento, bagging_fractiondeve ser configurado para valor menor que 1.0tamb√©m\n",
        "\n",
        "\n",
        "<hr>\n",
        "<hr>\n",
        "early_stopping_round üîóÔ∏é , default =0, type = int, aliases:early_stopping_rounds,early_stopping,n_iter_no_change\n",
        "\n",
        "    ir√° parar de treinar se uma m√©trica de um dado de valida√ß√£o n√£o melhorar nas √∫ltimas early_stopping_roundrodadas\n",
        "\n",
        "    <= 0significa desabilitar\n",
        "\n",
        "    pode ser usado para acelerar o treinamento\n",
        "\n",
        "\n",
        "<hr>\n",
        "extra_trees üîóÔ∏é , default =false, type = bool, aliases:extra_tree\n",
        "\n",
        "    use √°rvores extremamente aleat√≥rias\n",
        "\n",
        "    se definido como true, ao avaliar as divis√µes de n√≥s, o LightGBM verificar√° apenas um limite escolhido aleatoriamente para cada recurso\n",
        "\n",
        "    pode ser usado para acelerar o treinamento\n",
        "\n",
        "    pode ser usado para lidar com excesso de ajuste\n",
        "\n",
        "<hr>\n",
        "path_smooth üîóÔ∏é , default =0, type = double, restri√ß√µes:path_smooth >=  0.0\n",
        "\n",
        "    controla a suaviza√ß√£o aplicada aos n√≥s da √°rvore\n",
        "\n",
        "    ajuda a evitar overfitting em folhas com poucas amostras\n",
        "\n",
        "    se definido como zero, nenhuma suaviza√ß√£o √© aplicada\n",
        "\n",
        "    se ent√£o deve ser pelo menospath_smooth > 0min_data_in_leaf2\n",
        "\n",
        "    valores maiores d√£o uma regulariza√ß√£o mais forte\n",
        "\n",
        "        o peso de cada n√≥ √© , onde √© o n√∫mero de amostras no n√≥, √© o peso ideal do n√≥ para minimizar a perda (aproximadamente ), e √© o peso do n√≥ pai(n / path_smooth) * w + w_p / (n / path_smooth + 1)nw-sum_gradients / sum_hessiansw_p\n",
        "\n",
        "        observe que a pr√≥pria sa√≠da pai w_ptem suaviza√ß√£o aplicada, a menos que seja o n√≥ raiz, para que o efeito de suaviza√ß√£o se acumule com a profundidade da √°rvore\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rElKlXt_o2fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Par√¢metros de E/S\n",
        "\n",
        "<hr>\n",
        "max_bin üîóÔ∏é , default =255, type = int, aliases:max_bins, restri√ß√µes:max_bin > 1\n",
        "\n",
        "    n√∫mero m√°ximo de compartimentos nos quais os valores de recurso ser√£o agrupados\n",
        "\n",
        "    um pequeno n√∫mero de caixas pode reduzir a precis√£o do treinamento, mas pode aumentar a pot√™ncia geral (lidar com o ajuste excessivo)\n",
        "\n",
        "    LightGBM ir√° comprimir automaticamente a mem√≥ria de acordo com max_bin. Por exemplo, LightGBM usar√° uint8_tpara valor de recurso semax_bin=255\n",
        "<hr>\n",
        "zero_as_missing üîóÔ∏é , default =false, type = bool\n",
        "\n",
        "    defina isso para truetratar todos os zeros como valores ausentes (incluindo os valores n√£o mostrados em LibSVM / matrizes esparsas)\n",
        "\n",
        "    defina isso para falseusar napara representar valores ausentes\n",
        "<hr>\n",
        "two_round üîóÔ∏é , default =false, type = bool, aliases:two_round_loading,use_two_round_loading\n",
        "\n",
        "    defina isso para truese o arquivo de dados for muito grande para caber na mem√≥ria\n",
        "\n",
        "    por padr√£o, o LightGBM mapear√° o arquivo de dados para a mem√≥ria e carregar√° recursos da mem√≥ria. Isso fornecer√° uma velocidade de carregamento de dados mais r√°pida, mas pode causar um erro de falta de mem√≥ria quando o arquivo de dados for muito grande\n",
        "\n",
        "    Nota : funciona apenas no caso de carregar dados diretamente do arquivo de texto\n",
        "<hr>\n",
        "header üîóÔ∏é , default =false, type = bool, aliases:has_header\n",
        "\n",
        "    defina isso para truese os dados de entrada tiverem cabe√ßalho\n",
        "\n",
        "    Nota : funciona apenas no caso de carregar dados diretamente do arquivo de texto\n",
        "<hr>\n",
        "group_column üîóÔ∏é , default =\"\", type = int ou string, aliases:group,group_id,query_column,query,query_id\n",
        "\n",
        "    usado para especificar a coluna de ID de consulta/grupo\n",
        "\n",
        "    use n√∫mero para √≠ndice, por exemplo, query=0significa que column_0 √© o id da consulta\n",
        "\n",
        "    adicione um prefixo name:para o nome da coluna, por exemploquery=name:query_id\n",
        "\n",
        "    Nota : funciona apenas no caso de carregar dados diretamente do arquivo de texto\n",
        "\n",
        "    Nota : os dados devem ser agrupados por query_id, para mais informa√ß√µes, consulte Query Data\n",
        "\n",
        "    Nota : o √≠ndice come√ßa 0e n√£o conta a coluna do r√≥tulo quando o tipo de passagem √© int, por exemplo, quando o r√≥tulo √© column_0 e query_id √© column_1, o par√¢metro correto √©query=0\n",
        "<hr>\n",
        "ignore_column üîóÔ∏é , default =\"\", type = multi-int ou string, aliases:ignore_feature,blacklist\n",
        "\n",
        "    usado para especificar algumas colunas ignoradas no treinamento\n",
        "\n",
        "    use n√∫mero para √≠ndice, por exemplo, ignore_column=0,1,2significa que column_0, column_1 e column_2 ser√£o ignorados\n",
        "\n",
        "    adicione um prefixo name:para o nome da coluna, por exemplo, ignore_column=name:c1,c2,c3significa que c1, c2 e c3 ser√£o ignorados\n",
        "\n",
        "    Nota : funciona apenas no caso de carregar dados diretamente do arquivo de texto\n",
        "\n",
        "    Nota : o √≠ndice come√ßa 0e n√£o conta a coluna do r√≥tulo quando o tipo de passagem √©int\n",
        "\n",
        "    Nota : apesar do fato de que as colunas especificadas ser√£o completamente ignoradas durante o treinamento, elas ainda devem ter um formato v√°lido permitindo que o LightGBM carregue o arquivo com sucesso\n",
        "<hr>\n",
        "categorical_feature üîóÔ∏é , default =\"\", type = multi-int ou string, aliases:cat_feature,categorical_column,cat_column,categorical_features\n",
        "\n",
        "    usado para especificar recursos categ√≥ricos\n",
        "\n",
        "    use n√∫mero para √≠ndice, por exemplo, categorical_feature=0,1,2significa que column_0, column_1 e column_2 s√£o recursos categ√≥ricos\n",
        "\n",
        "    adicione um prefixo name:para o nome da coluna, por exemplo, categorical_feature=name:c1,c2,c3significa que c1, c2 e c3 s√£o recursos categ√≥ricos\n",
        "\n",
        "    Nota : suporta apenas categ√≥rico com inttipo (n√£o aplic√°vel para dados representados como pandas DataFrame no pacote Python)\n",
        "\n",
        "    Nota : o √≠ndice come√ßa 0e n√£o conta a coluna do r√≥tulo quando o tipo de passagem √©int\n",
        "\n",
        "    Nota : todos os valores devem ser menores que Int32.MaxValue(2147483647)\n",
        "\n",
        "    Nota : usar valores grandes pode consumir mem√≥ria. A regra de decis√£o da √°rvore funciona melhor quando os recursos categ√≥ricos s√£o apresentados por inteiros consecutivos come√ßando em zero\n",
        "\n",
        "    Nota : todos os valores negativos ser√£o tratados como valores ausentes\n",
        "\n",
        "    Nota : a sa√≠da n√£o pode ser restrita monotonicamente em rela√ß√£o a um recurso categ√≥rico\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "uTWkF7a6r97w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pra√¢metros Objetivos\n",
        "\n",
        "<b> Desbalanceados </b>\n",
        "\n",
        "\n",
        "<hr>\n",
        "is_unbalance üîóÔ∏é , default =false, type = bool, aliases:unbalance,unbalanced_sets\n",
        "\n",
        "    usado apenas em binarye multiclassovaaplica√ß√µes\n",
        "\n",
        "    defina isso para truese os dados de treinamento estiverem desequilibrados\n",
        "\n",
        "    Observa√ß√£o : embora habilitar isso deva aumentar a m√©trica de desempenho geral do seu modelo, tamb√©m resultar√° em estimativas ruins das probabilidades de classe individual\n",
        "\n",
        "    Nota : este par√¢metro n√£o pode ser usado ao mesmo tempo com scale_pos_weight, escolha apenas um deles\n",
        "<hr>\n",
        "scale_pos_weight üîóÔ∏é , default =1.0, type = double, restri√ß√µes:scale_pos_weight > 0.0\n",
        "\n",
        "    usado apenas em binarye multiclassovaaplica√ß√µes\n",
        "\n",
        "    peso dos r√≥tulos com classe positiva\n",
        "\n",
        "    Observa√ß√£o : embora habilitar isso deva aumentar a m√©trica de desempenho geral do seu modelo, tamb√©m resultar√° em estimativas ruins das probabilidades de classe individual\n",
        "\n",
        "    Nota : este par√¢metro n√£o pode ser usado ao mesmo tempo com is_unbalance, escolha apenas um deles\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>\n",
        "<hr>\n"
      ],
      "metadata": {
        "id": "KUKwpNLYtzGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Par√£metros de M√©tricas\n",
        "\n",
        "\n",
        "<hr>\n",
        "metric üîóÔ∏é , default =\"\", type = multi-enum, aliases:metrics,metric_types\n",
        "\n",
        "    m√©trica(s) a ser(em) avaliada(s) no(s) conjunto(s) de avalia√ß√£o\n",
        "\n",
        "        \"\"(string vazia ou n√£o especificada) significa que a m√©trica correspondente √† especificada objectiveser√° usada (isso √© poss√≠vel apenas para fun√ß√µes objetivas predefinidas, caso contr√°rio, nenhuma m√©trica de avalia√ß√£o ser√° adicionada)\n",
        "\n",
        "        \"None\"(string, n√£o um Nonevalor) significa que nenhuma m√©trica ser√° registrada, aliases: na, null,custom\n",
        "\n",
        "        l1, perda absoluta, aliases: mean_absolute_error, mae,regression_l1\n",
        "\n",
        "        l2, perda quadrada, aliases: mean_squared_error, mse, regression_l2,regression\n",
        "\n",
        "        rmse, perda de raiz quadrada, aliases: root_mean_squared_error,l2_root\n",
        "\n",
        "        quantile, regress√£o quant√≠lica\n",
        "\n",
        "        mape, perda MAPE , aliases:mean_absolute_percentage_error\n",
        "\n",
        "        huber, Perda de Huber\n",
        "\n",
        "        fair, Perda justa\n",
        "\n",
        "        poisson, probabilidade logar√≠tmica negativa para regress√£o de Poisson\n",
        "\n",
        "        gamma, probabilidade logar√≠tmica negativa para regress√£o gama\n",
        "\n",
        "        gamma_deviance, desvio residual para regress√£o gama\n",
        "\n",
        "        tweedie, probabilidade logar√≠tmica negativa para regress√£o de Tweedie\n",
        "\n",
        "        ndcg, NDCG , aliases: lambdarank, rank_xendcg, xendcg, xe_ndcg, xe_ndcg_mart,xendcg_mart\n",
        "\n",
        "        map, MAP , aliases:mean_average_precision\n",
        "\n",
        "        auc, AUC\n",
        "\n",
        "        average_precision, pontua√ß√£o de precis√£o m√©dia\n",
        "\n",
        "        binary_logloss, perda de log , aliases:binary\n",
        "\n",
        "        binary_error, para uma amostra: 0para classifica√ß√£o correta, 1para classifica√ß√£o de erros\n",
        "\n",
        "        auc_mu, AUC-mu\n",
        "\n",
        "        multi_logloss, perda de log para classifica√ß√£o multiclasse, aliases: multiclass, softmax, multiclassova, multiclass_ova, ova,ovr\n",
        "\n",
        "        multi_error, taxa de erro para classifica√ß√£o multiclasse\n",
        "\n",
        "        cross_entropy, entropia cruzada (com pesos lineares opcionais), aliases:xentropy\n",
        "\n",
        "        cross_entropy_lambda, entropia cruzada ‚Äúponderada por intensidade‚Äù, aliases:xentlambda\n",
        "\n",
        "        kullback_leibler, diverg√™ncia de Kullback-Leibler , aliases:kldiv\n",
        "\n"
      ],
      "metadata": {
        "id": "4wnFrftCuVVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yrC5AOphz6Qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ajuste de √Årvore\n",
        "\n",
        "<hr>\n",
        "Para obter bons resultados usando uma √°rvore foliar, estes s√£o alguns par√¢metros importantes:\n",
        "\n",
        "num_leaves. Este √© o principal par√¢metro para controlar a complexidade do modelo de √°rvore. Teoricamente, podemos definir para obter o mesmo n√∫mero de folhas que a √°rvore em profundidade. No entanto, essa convers√£o simples n√£o √© boa na pr√°tica. A raz√£o √© que uma √°rvore de folha √© tipicamente muito mais profunda do que uma √°rvore de profundidade para um n√∫mero fixo de folhas. A profundidade irrestrita pode induzir o ajuste excessivo. Assim, ao tentar afinar o , devemos deix√°-lo menor que . Por exemplo, quando a √°rvore de profundidade pode obter uma boa precis√£o, mas definir para pode causar um ajuste excessivo e defini-la para ou pode obter uma precis√£o melhor do que a de profundidade.num_leaves = 2^(max_depth)num_leaves2^(max_depth)max_depth=7num_leaves1277080\n",
        "\n",
        "\n",
        "min_data_in_leaf. Este √© um par√¢metro muito importante para evitar o ajuste excessivo em uma √°rvore foliar. Seu valor ideal depende do n√∫mero de amostras de treinamento e num_leaves. Defini-lo para um valor alto pode evitar o crescimento muito profundo de uma √°rvore, mas pode causar um ajuste insuficiente. Na pr√°tica, configur√°-lo para centenas ou milhares √© suficiente para um grande conjunto de dados.\n",
        "\n",
        "max_depth. Voc√™ tamb√©m pode usar max_depthpara limitar explicitamente a profundidade da √°rvore.\n"
      ],
      "metadata": {
        "id": "olJEFnYyxT4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gc2eXw9Gxwrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de Classifica√ß√£o Binaria"
      ],
      "metadata": {
        "id": "Ezg0jPMbz6wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "from pathlib import Path\n",
        " \n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "import lightgbm as lgb\n",
        " \n",
        "print('Loading data...')\n",
        "# load or create your dataset\n",
        "regression_example_dir = Path(__file__).absolute().parents[1] / 'regression'\n",
        "df_train = pd.read_csv(str(regression_example_dir / 'regression.train'), header=None, sep='\\t')\n",
        "df_test = pd.read_csv(str(regression_example_dir / 'regression.test'), header=None, sep='\\t')\n",
        " \n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        " \n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        " \n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        " 'boosting_type': 'gbdt',\n",
        " 'objective': 'regression',\n",
        " 'metric': {'l2', 'l1'},\n",
        " 'num_leaves': 31,\n",
        " 'learning_rate': 0.05,\n",
        " 'feature_fraction': 0.9,\n",
        " 'bagging_fraction': 0.8,\n",
        " 'bagging_freq': 5,\n",
        " 'verbose': 0\n",
        "}\n",
        " \n",
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=20,\n",
        " valid_sets=lgb_eval,\n",
        " callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
        " \n",
        "print('Saving model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        " \n",
        "print('Starting predicting...')\n",
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
        "# eval\n",
        "rmse_test = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f'The RMSE of prediction is: {rmse_test}')"
      ],
      "metadata": {
        "id": "Z5ICbatJz9_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo avan√ßado"
      ],
      "metadata": {
        "id": "4FGnwrK60g1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import copy\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        " \n",
        "import lightgbm as lgb\n",
        " \n",
        "print('Loading data...')\n",
        "# load or create your dataset\n",
        "binary_example_dir = Path(__file__).absolute().parents[1] / 'binary_classification'\n",
        "df_train = pd.read_csv(str(binary_example_dir / 'binary.train'), header=None, sep='\\t')\n",
        "df_test = pd.read_csv(str(binary_example_dir / 'binary.test'), header=None, sep='\\t')\n",
        "W_train = pd.read_csv(str(binary_example_dir / 'binary.train.weight'), header=None)[0]\n",
        "W_test = pd.read_csv(str(binary_example_dir / 'binary.test.weight'), header=None)[0]\n",
        " \n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        " \n",
        "num_train, num_feature = X_train.shape\n",
        " \n",
        "# create dataset for lightgbm\n",
        "# if you want to re-use data, remember to set free_raw_data=False\n",
        "lgb_train = lgb.Dataset(X_train, y_train,\n",
        " weight=W_train, free_raw_data=False)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,\n",
        " weight=W_test, free_raw_data=False)\n",
        " \n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        " 'boosting_type': 'gbdt',\n",
        " 'objective': 'binary',\n",
        " 'metric': 'binary_logloss',\n",
        " 'num_leaves': 31,\n",
        " 'learning_rate': 0.05,\n",
        " 'feature_fraction': 0.9,\n",
        " 'bagging_fraction': 0.8,\n",
        " 'bagging_freq': 5,\n",
        " 'verbose': 0\n",
        "}\n",
        " \n",
        "# generate feature names\n",
        "feature_name = [f'feature_{col}' for col in range(num_feature)]\n",
        " \n",
        "print('Starting training...')\n",
        "# feature_name and categorical_feature\n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " valid_sets=lgb_train, # eval training data\n",
        " feature_name=feature_name,\n",
        " categorical_feature=[21])\n",
        " \n",
        "print('Finished first 10 rounds...')\n",
        "# check feature name\n",
        "print(f'7th feature name is: {lgb_train.feature_name[6]}')\n",
        " \n",
        "print('Saving model...')\n",
        "# save model to file\n",
        "gbm.save_model('model.txt')\n",
        " \n",
        "print('Dumping model to JSON...')\n",
        "# dump model to JSON (and save to file)\n",
        "model_json = gbm.dump_model()\n",
        " \n",
        "with open('model.json', 'w+') as f:\n",
        " json.dump(model_json, f, indent=4)\n",
        " \n",
        "# feature names\n",
        "print(f'Feature names: {gbm.feature_name()}')\n",
        " \n",
        "# feature importances\n",
        "print(f'Feature importances: {list(gbm.feature_importance())}')\n",
        " \n",
        "print('Loading model to predict...')\n",
        "# load model to predict\n",
        "bst = lgb.Booster(model_file='model.txt')\n",
        "# can only predict with the best iteration (or the saving iteration)\n",
        "y_pred = bst.predict(X_test)\n",
        "# eval with loaded model\n",
        "rmse_loaded_model = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f\"The RMSE of loaded model's prediction is: {rmse_loaded_model}\")\n",
        " \n",
        "print('Dumping and loading model with pickle...')\n",
        "# dump model with pickle\n",
        "with open('model.pkl', 'wb') as fout:\n",
        " pickle.dump(gbm, fout)\n",
        "# load model with pickle to predict\n",
        "with open('model.pkl', 'rb') as fin:\n",
        " pkl_bst = pickle.load(fin)\n",
        "# can predict with any iteration when loaded in pickle way\n",
        "y_pred = pkl_bst.predict(X_test, num_iteration=7)\n",
        "# eval with loaded model\n",
        "rmse_pickled_model = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f\"The RMSE of pickled model's prediction is: {rmse_pickled_model}\")\n",
        " \n",
        "# continue training\n",
        "# init_model accepts:\n",
        "# 1. model file name\n",
        "# 2. Booster()\n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " init_model='model.txt',\n",
        " valid_sets=lgb_eval)\n",
        " \n",
        "print('Finished 10 - 20 rounds with model file...')\n",
        " \n",
        "# decay learning rates\n",
        "# reset_parameter callback accepts:\n",
        "# 1. list with length = num_boost_round\n",
        "# 2. function(curr_iter)\n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " init_model=gbm,\n",
        " valid_sets=lgb_eval,\n",
        " callbacks=[lgb.reset_parameter(learning_rate=lambda iter: 0.05 * (0.99 ** iter))])\n",
        " \n",
        "print('Finished 20 - 30 rounds with decay learning rates...')\n",
        " \n",
        "# change other parameters during training\n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " init_model=gbm,\n",
        " valid_sets=lgb_eval,\n",
        " callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])\n",
        " \n",
        "print('Finished 30 - 40 rounds with changing bagging_fraction...')\n",
        " \n",
        " \n",
        "# self-defined objective function\n",
        "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
        "# log likelihood loss\n",
        "def loglikelihood(preds, train_data):\n",
        " labels = train_data.get_label()\n",
        " preds = 1. / (1. + np.exp(-preds))\n",
        " grad = preds - labels\n",
        " hess = preds * (1. - preds)\n",
        " return grad, hess\n",
        " \n",
        " \n",
        "# self-defined eval metric\n",
        "# f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool\n",
        "# binary error\n",
        "# NOTE: when you do customized loss function, the default prediction value is margin\n",
        "# This may make built-in evaluation metric calculate wrong results\n",
        "# For example, we are doing log likelihood loss, the prediction is score before logistic transformation\n",
        "# Keep this in mind when you use the customization\n",
        "def binary_error(preds, train_data):\n",
        " labels = train_data.get_label()\n",
        " preds = 1. / (1. + np.exp(-preds))\n",
        " return 'error', np.mean(labels != (preds > 0.5)), False\n",
        " \n",
        " \n",
        "# Pass custom objective function through params\n",
        "params_custom_obj = copy.deepcopy(params)\n",
        "params_custom_obj['objective'] = loglikelihood\n",
        " \n",
        "gbm = lgb.train(params_custom_obj,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " init_model=gbm,\n",
        " feval=binary_error,\n",
        " valid_sets=lgb_eval)\n",
        " \n",
        "print('Finished 40 - 50 rounds with self-defined objective function and eval metric...')\n",
        " \n",
        " \n",
        "# another self-defined eval metric\n",
        "# f(preds: array, train_data: Dataset) -> name: str, eval_result: float, is_higher_better: bool\n",
        "# accuracy\n",
        "# NOTE: when you do customized loss function, the default prediction value is margin\n",
        "# This may make built-in evaluation metric calculate wrong results\n",
        "# For example, we are doing log likelihood loss, the prediction is score before logistic transformation\n",
        "# Keep this in mind when you use the customization\n",
        "def accuracy(preds, train_data):\n",
        " labels = train_data.get_label()\n",
        " preds = 1. / (1. + np.exp(-preds))\n",
        " return 'accuracy', np.mean(labels == (preds > 0.5)), True\n",
        " \n",
        " \n",
        "# Pass custom objective function through params\n",
        "params_custom_obj = copy.deepcopy(params)\n",
        "params_custom_obj['objective'] = loglikelihood\n",
        " \n",
        "gbm = lgb.train(params_custom_obj,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " init_model=gbm,\n",
        " feval=[binary_error, accuracy],\n",
        " valid_sets=lgb_eval)\n",
        " \n",
        "print('Finished 50 - 60 rounds with self-defined objective function and multiple self-defined eval metrics...')\n",
        " \n",
        "print('Starting a new training job...')\n",
        " \n",
        " \n",
        "# callback\n",
        "def reset_metrics():\n",
        " def callback(env):\n",
        " lgb_eval_new = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        " if env.iteration - env.begin_iteration == 5:\n",
        " print('Add a new valid dataset at iteration 5...')\n",
        " env.model.add_valid(lgb_eval_new, 'new_valid')\n",
        " callback.before_iteration = True\n",
        " callback.order = 0\n",
        " return callback\n",
        " \n",
        " \n",
        "gbm = lgb.train(params,\n",
        " lgb_train,\n",
        " num_boost_round=10,\n",
        " valid_sets=lgb_train,\n",
        " callbacks=[reset_metrics()])\n",
        " \n",
        "print('Finished first 10 rounds with callback function...')"
      ],
      "metadata": {
        "id": "X5yE_HI-0iXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de Plot"
      ],
      "metadata": {
        "id": "9poUEreg0mM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "from pathlib import Path\n",
        " \n",
        "import pandas as pd\n",
        " \n",
        "import lightgbm as lgb\n",
        " \n",
        "if lgb.compat.MATPLOTLIB_INSTALLED:\n",
        " import matplotlib.pyplot as plt\n",
        "else:\n",
        " raise ImportError('You need to install matplotlib and restart your session for plot_example.py.')\n",
        " \n",
        "print('Loading data...')\n",
        "# load or create your dataset\n",
        "regression_example_dir = Path(__file__).absolute().parents[1] / 'regression'\n",
        "df_train = pd.read_csv(str(regression_example_dir / 'regression.train'), header=None, sep='\\t')\n",
        "df_test = pd.read_csv(str(regression_example_dir / 'regression.test'), header=None, sep='\\t')\n",
        " \n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        " \n",
        "# create dataset for lightgbm\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        " \n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        " 'num_leaves': 5,\n",
        " 'metric': ('l1', 'l2'),\n",
        " 'verbose': 0\n",
        "}\n",
        " \n",
        "evals_result = {} # to record eval results for plotting\n",
        " \n",
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.train(\n",
        " params,\n",
        " lgb_train,\n",
        " num_boost_round=100,\n",
        " valid_sets=[lgb_train, lgb_test],\n",
        " feature_name=[f'f{i + 1}' for i in range(X_train.shape[-1])],\n",
        " categorical_feature=[21],\n",
        " callbacks=[\n",
        " lgb.log_evaluation(10),\n",
        " lgb.record_evaluation(evals_result)\n",
        " ]\n",
        ")\n",
        " \n",
        "print('Plotting metrics recorded during training...')\n",
        "ax = lgb.plot_metric(evals_result, metric='l1')\n",
        "plt.show()\n",
        " \n",
        "print('Plotting feature importances...')\n",
        "ax = lgb.plot_importance(gbm, max_num_features=10)\n",
        "plt.show()\n",
        " \n",
        "print('Plotting split value histogram...')\n",
        "ax = lgb.plot_split_value_histogram(gbm, feature='f26', bins='auto')\n",
        "plt.show()\n",
        " \n",
        "print('Plotting 54th tree...') # one tree use categorical feature to split\n",
        "ax = lgb.plot_tree(gbm, tree_index=53, figsize=(15, 15), show_info=['split_gain'])\n",
        "plt.show()\n",
        " \n",
        "print('Plotting 54th tree with graphviz...')\n",
        "graph = lgb.create_tree_digraph(gbm, tree_index=53, name='Tree54')\n",
        "graph.render(view=True)"
      ],
      "metadata": {
        "id": "b8q14OVN0nyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo com SKlearn"
      ],
      "metadata": {
        "id": "npfiwTJB0s0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "from pathlib import Path\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "import lightgbm as lgb\n",
        " \n",
        "print('Loading data...')\n",
        "# load or create your dataset\n",
        "regression_example_dir = Path(__file__).absolute().parents[1] / 'regression'\n",
        "df_train = pd.read_csv(str(regression_example_dir / 'regression.train'), header=None, sep='\\t')\n",
        "df_test = pd.read_csv(str(regression_example_dir / 'regression.test'), header=None, sep='\\t')\n",
        " \n",
        "y_train = df_train[0]\n",
        "y_test = df_test[0]\n",
        "X_train = df_train.drop(0, axis=1)\n",
        "X_test = df_test.drop(0, axis=1)\n",
        " \n",
        "print('Starting training...')\n",
        "# train\n",
        "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
        " learning_rate=0.05,\n",
        " n_estimators=20)\n",
        "gbm.fit(X_train, y_train,\n",
        " eval_set=[(X_test, y_test)],\n",
        " eval_metric='l1',\n",
        " callbacks=[lgb.early_stopping(5)])\n",
        " \n",
        "print('Starting predicting...')\n",
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
        "# eval\n",
        "rmse_test = mean_squared_error(y_test, y_pred) ** 0.5\n",
        "print(f'The RMSE of prediction is: {rmse_test}')\n",
        " \n",
        "# feature importances\n",
        "print(f'Feature importances: {list(gbm.feature_importances_)}')\n",
        " \n",
        " \n",
        "# self-defined eval metric\n",
        "# f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool\n",
        "# Root Mean Squared Logarithmic Error (RMSLE)\n",
        "def rmsle(y_true, y_pred):\n",
        " return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n",
        " \n",
        " \n",
        "print('Starting training with custom eval function...')\n",
        "# train\n",
        "gbm.fit(X_train, y_train,\n",
        " eval_set=[(X_test, y_test)],\n",
        " eval_metric=rmsle,\n",
        " callbacks=[lgb.early_stopping(5)])\n",
        " \n",
        " \n",
        "# another self-defined eval metric\n",
        "# f(y_true: array, y_pred: array) -> name: str, eval_result: float, is_higher_better: bool\n",
        "# Relative Absolute Error (RAE)\n",
        "def rae(y_true, y_pred):\n",
        " return 'RAE', np.sum(np.abs(y_pred - y_true)) / np.sum(np.abs(np.mean(y_true) - y_true)), False\n",
        " \n",
        " \n",
        "print('Starting training with multiple custom eval functions...')\n",
        "# train\n",
        "gbm.fit(X_train, y_train,\n",
        " eval_set=[(X_test, y_test)],\n",
        " eval_metric=[rmsle, rae],\n",
        " callbacks=[lgb.early_stopping(5)])\n",
        " \n",
        "print('Starting predicting...')\n",
        "# predict\n",
        "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
        "# eval\n",
        "rmsle_test = rmsle(y_test, y_pred)[1]\n",
        "rae_test = rae(y_test, y_pred)[1]\n",
        "print(f'The RMSLE of prediction is: {rmsle_test}')\n",
        "print(f'The RAE of prediction is: {rae_test}')\n",
        " \n",
        "# other scikit-learn modules\n",
        "estimator = lgb.LGBMRegressor(num_leaves=31)\n",
        " \n",
        "param_grid = {\n",
        " 'learning_rate': [0.01, 0.1, 1],\n",
        " 'n_estimators': [20, 40]\n",
        "}\n",
        " \n",
        "gbm = GridSearchCV(estimator, param_grid, cv=3)\n",
        "gbm.fit(X_train, y_train)\n",
        " \n",
        "print(f'Best parameters found by grid search are: {gbm.best_params_}')"
      ],
      "metadata": {
        "id": "COa0ZX4t0uyr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}